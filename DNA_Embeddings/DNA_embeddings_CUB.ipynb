{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 15:52:05.550523: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-22 15:52:05.654440: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-22 15:52:06.058344: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-22 15:52:06.058387: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-22 15:52:06.058393: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "datapath = r'../data'\n",
    "dataset = 'CUB' # This is the same CUB data, just 6 classes lacking any DNA barcodes are removed \n",
    "x2 = sio.loadmat(os.path.join(datapath, dataset, 'CUB_DNA.mat')) \n",
    "x = sio.loadmat(os.path.join(datapath, dataset, 'Bird_DNA.mat'))  # Dataset containing all bird DNA barcodes extracted from COI gene\n",
    "us_cls = sio.loadmat(os.path.join(datapath, dataset, 'CUB_unseen_classes_sci_name.mat'))\n",
    "barcodes=x['nucleotides_aligned'][0]\n",
    "unseen_cls = us_cls['us_classes']\n",
    "bird_species=x['species'][0]\n",
    "cub_barcodes = x2['nucleotides_aligned'][0]\n",
    "c_labels = x2['species'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26409\n"
     ]
    }
   ],
   "source": [
    "# Number of training samples and entire data\n",
    "N = len(barcodes)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcodes=[]\n",
    "#labels=[]\n",
    "b_species = []\n",
    "for i in range(N):\n",
    "    if len(barcodes[i][0])>0:\n",
    "        bcodes.append(barcodes[i][0])\n",
    "        b_species.append(bird_species[i][0])\n",
    "\n",
    "b_species = np.asarray(b_species)\n",
    "bcodes = np.asarray(bcodes)\n",
    "        \n",
    "# c_species = []\n",
    "# for i in range(len(cub_species)):\n",
    "#     c_species.append(cub_species[i][0][0].split('--> ')[1])  \n",
    "    \n",
    "cub_bcodes = []\n",
    "c_labels_ = []\n",
    "for i in range(len(cub_barcodes)):\n",
    "    cub_bcodes.append(cub_barcodes[i][0])\n",
    "    c_labels_.append(c_labels[i][0])\n",
    "    \n",
    "unseen_classes = []\n",
    "for i in range(len(unseen_cls)):\n",
    "    unseen_classes.append(unseen_cls[i][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample barcode: A-----------------------------------------------------CTATACCTAATCTTCGGCGCCTGAGCTGGTATAGTCGGCACCGCCCTCAGCCTACTCATTCGCGCAGAACTCGGCCAACCAGGCACACTCCTAGGCGACGACCAAATCTACAACGTAATCGTTACTGCACATGCCTTTGTAATAATCTTCTTCATAGTTATACCAATCATGATTGGAGGATTCGGAAACTGACTTGTCCCACTCATAATTGGAGCCCCCGACATAGCCTTTCCACGCATAAACAACATAAGCTTCTGGCTACTCCCTCCATCCTTCCTCCTCCTACTGGCCTCCTCAACAGTAGAAGCAGGAGCCGGCACTGGATGAACTGTCTACCCCCCATTAGCTGGCAACATAGCCCATGCCGGAGCTTCAGTAGACTTGGCCATCTTCTCCTTACACTTAGCTGGAGTCTCATCCATTCTAGGAGCAATCAACTTTATCACAACCGCCATCAACATAAAACCCCCAGCCCTCTCCCAGTACCAAACACCCCTATTCGTATGATCTGTCCTCATTACCGCCGTCCTTCTATTACTTTCACTCCCAGTCCTAGCCGCTGGCATCACTATACTACTCACAGACCGAAACCTAAACACAACATTCTTTGACCCTGCCGGCGGAGGCGATCCTATCCTATACCAACACCTCTTCTGATTCTTTGGACACCCAGAAGTCTACATCTTAATCCT---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------A\n"
     ]
    }
   ],
   "source": [
    "print('Sample barcode: %s' % bcodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25712 697\n"
     ]
    }
   ],
   "source": [
    "# Excluding unseen classes of CUB data from training set\n",
    "idx = np.in1d(b_species, unseen_classes)\n",
    "\n",
    "b_species = b_species[~idx]\n",
    "bcodes = bcodes[~idx]\n",
    "print(len(b_species), sum(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 999\n"
     ]
    }
   ],
   "source": [
    "uy = np.unique(b_species)\n",
    "labels = np.zeros(len(b_species), dtype='int32')\n",
    "for i in range(len(uy)):\n",
    "    idx = b_species==uy[i]\n",
    "    labels[idx.ravel()] = i+1 # Matlab indexing\n",
    "    if idx.sum()<10:\n",
    "        print(uy[i])\n",
    "    \n",
    "print('Number of classes: %d' % len(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aechmophorus occidentalis\n",
      "Aethia cristatella\n",
      "Aethia psittacula\n",
      "Aethia pusilla\n",
      "Ammodramus bairdii\n",
      "Ammodramus henslowii\n",
      "Ammodramus leconteii\n",
      "Ammodramus maritimus\n",
      "Ammodramus savannarum\n",
      "Amphispiza bilineata\n",
      "Antrostomus carolinensis\n",
      "Antrostomus vociferus\n",
      "Aphelocoma coerulescens\n",
      "Bombycilla cedrorum\n",
      "Calypte anna\n",
      "Campylorhynchus brunneicapillus\n",
      "Cepphus columba\n",
      "Ceryle rudis\n",
      "Chordeiles minor\n",
      "Coccyzus americanus\n",
      "Coccyzus erythropthalmus\n",
      "Coccyzus minor\n",
      "Colibri thalassinus\n",
      "Contopus cooperi\n",
      "Corvus albicollis\n",
      "Corvus ossifragus\n",
      "Cyanocitta cristata\n",
      "Cyanocorax yncas\n",
      "Dolichonyx oryzivorus\n",
      "Dryocopus pileatus\n",
      "Empidonax flaviventris\n",
      "Empidonax minimus\n",
      "Empidonax virescens\n",
      "Euphagus carolinus\n",
      "Fratercula corniculata\n",
      "Geococcyx californianus\n",
      "Helmitheros vermivorum\n",
      "Icteria virens\n",
      "Icterus cucullatus\n",
      "Icterus parisorum\n",
      "Icterus spurius\n",
      "Lamprotornis nitens\n",
      "Lanius ludovicianus\n",
      "Larus californicus\n",
      "Larus delawarensis\n",
      "Larus glaucescens\n",
      "Larus heermanni\n",
      "Larus occidentalis\n",
      "Leiothlypis ruficapilla\n",
      "Leuconotopicus borealis\n",
      "Leucosticte tephrocotis\n",
      "Limnothlypis swainsonii\n",
      "Megaceryle alcyon\n",
      "Melanerpes erythrocephalus\n",
      "Myiarchus crinitus\n",
      "Oreoscoptes montanus\n",
      "Pagophila eburnea\n",
      "Parkesia motacilla\n",
      "Passerina caerulea\n",
      "Passerina ciris\n",
      "Passerina cyanea\n",
      "Pelecanus erythrorhynchos\n",
      "Pelecanus occidentalis\n",
      "Petrochelidon pyrrhonota\n",
      "Phalacrocorax penicillatus\n",
      "Phalacrocorax urile\n",
      "Pheucticus ludovicianus\n",
      "Picoides dorsalis\n",
      "Pipilo chlorurus\n",
      "Pipilo erythrophthalmus\n",
      "Piranga olivacea\n",
      "Podiceps auritus\n",
      "Podiceps nigricollis\n",
      "Podilymbus podiceps\n",
      "Pooecetes gramineus\n",
      "Quiscalus major\n",
      "Rissa brevirostris\n",
      "Salpinctes obsoletus\n",
      "Sayornis phoebe\n",
      "Selasphorus rufus\n",
      "Setophaga cerulea\n",
      "Setophaga citrina\n",
      "Setophaga discolor\n",
      "Setophaga pensylvanica\n",
      "Setophaga pinus\n",
      "Setophaga tigrina\n",
      "Sitta carolinensis\n",
      "Spizella pallida\n",
      "Spizella pusilla\n",
      "Sterna forsteri\n",
      "Sternula antillarum\n",
      "Sturnella neglecta\n",
      "Thalasseus elegans\n",
      "Thryothorus ludovicianus\n",
      "Troglodytes hiemalis\n",
      "Tyrannus dominicensis\n",
      "Tyrannus forficatus\n",
      "Vermivora chrysoptera\n",
      "Vermivora cyanoptera\n",
      "Vireo atricapilla\n",
      "Vireo flavifrons\n",
      "Xanthocephalus xanthocephalus\n",
      "Zonotrichia querula\n",
      "194 [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n"
     ]
    }
   ],
   "source": [
    "# Classes and their names from CUB data that has less than 10 samples\n",
    "# Just for information\n",
    "uy = np.unique(c_labels_)\n",
    "c_labels_ = np.asarray(c_labels_)\n",
    "c_labels = np.zeros(len(c_labels_), dtype='int32')\n",
    "for i in range(len(uy)):\n",
    "    idx = c_labels_==uy[i]\n",
    "    c_labels[idx] = i+1 # Matlab indexing\n",
    "    if np.sum(idx)<10:\n",
    "        print(uy[i])\n",
    "    \n",
    "print(len(np.unique(c_labels)), np.unique(c_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# Converting base pairs, ACTG, into numbers: Tokenization\n",
    "\n",
    "### For BIRD data\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(bcodes)\n",
    "sequence_of_int = tokenizer.texts_to_sequences(bcodes)\n",
    "\n",
    "### For CUB data barcodes\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(cub_bcodes)\n",
    "sequence_of_int_cub = tokenizer.texts_to_sequences(cub_bcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1548.0\n"
     ]
    }
   ],
   "source": [
    "# Bird data barcode lengths have a high variance and not as in INSECT data majority has seq length of 658\n",
    "# These barcodes are aligned ones as in INSECT data\n",
    "cnt = 0\n",
    "ll = np.zeros((len(bcodes), 1))\n",
    "for i in range(len(bcodes)):\n",
    "    ll[i] = len(sequence_of_int[i])\n",
    "    if ll[i]==658:\n",
    "        #print(i)\n",
    "        cnt += 1\n",
    "print(cnt, np.median(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculating sequence count for each species\n",
    "cl = len(np.unique(b_species))\n",
    "N = len(b_species)\n",
    "seq_len=np.zeros((cl))\n",
    "seq_cnt=np.zeros((cl))\n",
    "for i in range(N):\n",
    "    k=labels[i]-1\n",
    "    seq_cnt[k]+=1\n",
    "    #seq_len[k]+=len(sequence_of_int[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# classes: 999\n"
     ]
    }
   ],
   "source": [
    "print('# classes: %d' % cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the all CUB DNA barcodes into one-hot encoding. Note that this data is not used during training. \n",
    "# We are getting it ready for the prediction time to get the final DNA embeddings after training is done\n",
    "\n",
    "sl = 1500 # Fixed to the max sequnce length. Note that, changing it to median value, 1548, does have an infitesimal effect\n",
    "N_cub = len(cub_bcodes)\n",
    "allX=np.zeros((N_cub,sl,5))\n",
    "for i in range(N_cub):\n",
    "    Nt=len(sequence_of_int_cub[i])\n",
    "\n",
    "    for j in range(sl):\n",
    "        if(len(sequence_of_int_cub[i])>j):\n",
    "            k=sequence_of_int_cub[i][j]-1\n",
    "            if(k>4):\n",
    "                k=4\n",
    "            allX[i][j][k]=1.0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the training matrix and labels\n",
    "trainX=np.zeros((N,sl,5))\n",
    "trainY=np.zeros((N,cl))\n",
    "labelY=np.zeros(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where we cap the class sample size to 50 for a balanced training set\n",
    "\n",
    "Nc=-1\n",
    "clas_cnt=np.zeros((cl))\n",
    "for i in range(N):\n",
    "    Nt=len(sequence_of_int[i])\n",
    "    k=labels[i]-1\n",
    "    clas_cnt[k]+=1\n",
    "    itl=i+1\n",
    "    if(seq_cnt[k]>=10 and clas_cnt[k]<=50):\n",
    "        Nc=Nc+1\n",
    "        for j in range(sl):\n",
    "            if(len(sequence_of_int[i])>j):\n",
    "                k=sequence_of_int[i][j]-1\n",
    "                if(k>4):\n",
    "                    k=4\n",
    "                trainX[Nc][j][k]=1.0\n",
    "            \n",
    "        k=labels[i]-1\n",
    "        trainY[Nc][k]=1.0\n",
    "        labelY[Nc]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training size: 21230\n"
     ]
    }
   ],
   "source": [
    "print('Final training size: %d' % Nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21230, 1500, 5) (21230, 999) (21230,)\n"
     ]
    }
   ],
   "source": [
    "# selecting the balanced training set\n",
    "trainX=trainX[0:Nc]\n",
    "trainY=trainY[0:Nc]\n",
    "labelY=labelY[0:Nc]\n",
    "print(trainX.shape, trainY.shape, labelY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10.0\n"
     ]
    }
   ],
   "source": [
    "idx = np.argwhere(np.all(trainY[..., :] == 0, axis=0))\n",
    "trainY = np.delete(trainY, idx, axis=1)\n",
    "\n",
    "print(len(idx), min(np.sum(trainY,axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21230, 999)\n",
      "(21230,)\n"
     ]
    }
   ],
   "source": [
    "print(trainY.shape)\n",
    "print(labelY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21230, 1500, 5, 1) (2772, 1500, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "# Expanding the training set shape for CNN \n",
    "trainX=np.expand_dims(trainX, axis=3)\n",
    "allX=np.expand_dims(allX, axis=3)\n",
    "print(trainX.shape, allX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the training set into train and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainX, trainY, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16984, 999)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models, optimizers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 15:52:33.063740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-22 15:52:33.069839: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-10-22 15:52:33.069851: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-10-22 15:52:33.070190: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# CNN model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu', input_shape=(sl, 5,1),padding=\"SAME\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((3,1)))\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu',padding=\"SAME\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((3,1)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu',padding=\"SAME\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((3,1)))\n",
    "# model.add(layers.Conv2D(16, (3,3), activation='relu',padding=\"SAME\"))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((3,1)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(400, activation='tanh'))\n",
    "model.add(layers.Dense(y_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 1500, 5, 64)       640       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 1500, 5, 64)      256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 500, 5, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 500, 5, 32)        18464     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 500, 5, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 166, 5, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 166, 5, 16)        4624      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 166, 5, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 55, 5, 16)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4400)              0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 4400)             17600     \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 400)               1760400   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 999)               400599    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,202,775\n",
      "Trainable params: 2,193,751\n",
      "Non-trainable params: 9,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-decay learning rate scheduler\n",
    "def step_decay(epoch):\n",
    "   initial_lrate = 0.001\n",
    "   drop = 0.5\n",
    "   epochs_drop = 2.0\n",
    "   lrate = initial_lrate * np.power(drop, np.floor((1+epoch)/epochs_drop))\n",
    "   return lrate\n",
    "\n",
    "class LossHistory(callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "       self.losses = []\n",
    "       self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "       self.losses.append(logs.get('loss'))\n",
    "       self.lr.append(step_decay(len(self.losses)))\n",
    "        \n",
    "loss_history = LossHistory()\n",
    "lrate = callbacks.LearningRateScheduler(step_decay)\n",
    "callbacks_list = [loss_history, lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "531/531 [==============================] - 98s 184ms/step - loss: 1.6566 - accuracy: 0.7677 - top_k_categorical_accuracy: 0.8610 - val_loss: 4.1108 - val_accuracy: 0.3789 - val_top_k_categorical_accuracy: 0.6368 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "318/531 [================>.............] - ETA: 38s - loss: 0.1940 - accuracy: 0.9564 - top_k_categorical_accuracy: 0.9960"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_478403/2300049641.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Validation time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m history = model.fit(X_train, y_train, epochs=5, batch_size = 32, validation_data=(X_test, y_test),\n\u001b[0;32m----> 8\u001b[0;31m                       callbacks=callbacks_list, verbose=1)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Final Test time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bzsl/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bzsl/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1562\u001b[0m                         ):\n\u001b[1;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1564\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1565\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bzsl/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bzsl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bzsl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bzsl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2496\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2497\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bzsl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1861\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1863\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bzsl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/bzsl/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "#opt = tf.keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy','top_k_categorical_accuracy'])\n",
    "# Validation time\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size = 32, validation_data=(X_test, y_test),\n",
    "                      callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "# Final Test time\n",
    "#history = model.fit(trainX, trainY, epochs=5, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Learning rates through epochs:', loss_history.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving and Loading model\n",
    "model.save('CUB_DNA_5e_adam_aligned')\n",
    "model = tf.keras.models.load_model('CUB_DNA_5e_adam_aligned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "# Getting the DNA embeddings of all data from the last dense layer\n",
    "layer_name = 'dense'\n",
    "model2= Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "dna_embeddings=model2.predict(allX)\n",
    "print(dna_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"cub_cnn_embeddings_400_5e_adam_aligned.csv\", dna_embeddings, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
